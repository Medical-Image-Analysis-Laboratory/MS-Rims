{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration...\n",
      "Configuration loaded successfully!\n",
      "_____________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from config import *\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "from utils import generate_BIDS_path, get_dataframe_from_split_lesions, load_lesions\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = \"morpho_v02_maxDA\"\n",
    "MARGIN_SIZE = 11\n",
    "DILATIONS_NUM = 5\n",
    "VOLUME_FILTER = 45\n",
    "DIM_FILTER = 24\n",
    "MIN_PERCENTAGE_FOR_RIM_POS_PATCHES = 70\n",
    "RANDOM_DA = 0. # old value: 0.4, the higher, the less DA there is. Value between 0 (maximum DA) and 1 (NO DA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_for_split_lesions(dataset_id, pat, der_folder, pipeline, session=1):\n",
    "    # we create folder if it does not exist\n",
    "    der_path = os.path.join(AVAILABLE_DATASETS_ROOTS[dataset_id], \"derivatives\", der_folder)\n",
    "    if not os.path.exists(der_path):\n",
    "        try:\n",
    "            os.makedirs(der_path)\n",
    "            print(f\"[INFO] Derivatives folder for '{der_folder}' successfully created.\")\n",
    "        except: # Sometimes in multiprocessing this check is true for several processes and crashes\n",
    "            pass\n",
    "        \n",
    "    # we create the description of the derivatives if it does not exist\n",
    "    dataset_description_path = os.path.join(der_path, \"dataset_description.json\")\n",
    "    if not os.path.exists(os.path.join(dataset_description_path)):\n",
    "        descriptor = {\n",
    "            \"Name\": der_folder,\n",
    "            \"BIDSVersion\": BIDS_VERSION,\n",
    "            \"PipelineDescription\": {\n",
    "                \"Name\": pipeline,\n",
    "                \"version\": VERSION,\n",
    "                \"margin_size\": MARGIN_SIZE,\n",
    "                \"dilations_num\": DILATIONS_NUM,\n",
    "                \"volume_filter\": VOLUME_FILTER,\n",
    "                \"dim_filter\": DIM_FILTER,\n",
    "                \"min_percentage_for_rim_pos_patches\": MIN_PERCENTAGE_FOR_RIM_POS_PATCHES,\n",
    "                \"random_DA\": RANDOM_DA,\n",
    "            }\n",
    "        }\n",
    "        with open(dataset_description_path, \"w\") as outfile:\n",
    "            json.dump(descriptor, outfile)\n",
    "        print(f\"[INFO] Description file for '{der_folder}' successfully created.\")\n",
    "    \n",
    "    # we create the path for the generated file\n",
    "    folder = os.path.join(der_path, f\"sub-{pat:03d}\", f\"ses-{session:02d}\")\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    return folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def __extract_patch(mask):\n",
    "    slice_x, slice_y, slice_z = ndimage.find_objects(mask)[0]\n",
    "    roi = mask[slice_x, slice_y, slice_z]\n",
    "    return roi, [slice_x.start, slice_y.start, slice_z.start]\n",
    "\n",
    "def __get_and_remove_patches(original, lesions_guesses):\n",
    "    centers = []\n",
    "    result = original.copy()\n",
    "    \n",
    "    s = np.ones((3, 3, 3))\n",
    "    labelled, num_objects = ndimage.measurements.label(lesions_guesses, structure=s)\n",
    "    \n",
    "    for l in range(1, num_objects+1):\n",
    "        obj = (labelled == l).astype(int)\n",
    "        com = [int(round(v)) for v in ndimage.measurements.center_of_mass(obj)]\n",
    "        centers.append(com)\n",
    "        \n",
    "        ext = MARGIN_SIZE # SPHERE better??\n",
    "        (x0, y0, z0) = com - np.array((ext, ext, ext))\n",
    "        (x, y, z) = com + np.array((ext, ext, ext))\n",
    "        lim = original.shape\n",
    "        result[max(0, x0) : min(x, lim[0]), max(0, y0) : min(y, lim[1]), max(0, z0) : min(z, lim[2])] = 0\n",
    "        \n",
    "    return result, centers\n",
    "\n",
    "def __paint_patch(mask, center, les_id, ext=MARGIN_SIZE):\n",
    "    (x0, y0, z0) = center - np.array((ext, ext, ext))\n",
    "    (x, y, z) = center + np.array((ext, ext, ext))\n",
    "    lim = mask.shape\n",
    "    mask[max(0, x0) : min(x, lim[0]), max(0, y0) : min(y, lim[1]), max(0, z0) : min(z, lim[2])] = les_id\n",
    "    \n",
    "def __get_centers_of_sublesions(one_lesion_mask):\n",
    "    one_lesion_mask = one_lesion_mask.astype('int8')\n",
    "    \n",
    "    lesion_mask, pos0 = __extract_patch(one_lesion_mask)\n",
    "    all_centers = []\n",
    "    while(True):\n",
    "        dt_mask = np.round(ndimage.morphology.distance_transform_edt(lesion_mask)).astype(int)\n",
    "        max_val = np.max(dt_mask)\n",
    "        if max_val <= 1:\n",
    "            break\n",
    "        aux = ndimage.morphology.binary_dilation((dt_mask >= max_val).astype(int), iterations=DILATIONS_NUM)\n",
    "        \n",
    "        lesion_mask, centers = __get_and_remove_patches(lesion_mask, aux)\n",
    "        all_centers += [(c[0] + pos0[0], c[1] + pos0[1], c[2] + pos0[2]) for c in centers]\n",
    "        \"\"\"\n",
    "        # check for small parts to remove\n",
    "        labelled, num_objects = ndimage.measurements.label(lesion_mask)\n",
    "        for l in range(1, num_objects+1):\n",
    "            obj = (labelled == l).astype(int)\n",
    "            vol = np.sum(obj)\n",
    "            if vol <= VOLUME_FILTER:\n",
    "                lesion_mask[obj.astype(bool)] = 0\n",
    "                \"\"\"\n",
    "    \n",
    "    all_centers.reverse()\n",
    "    \n",
    "    counter = 0\n",
    "    patches = one_lesion_mask.astype(int)\n",
    "    for center in all_centers:\n",
    "        __paint_patch(patches, center, counter)\n",
    "        counter += 1\n",
    "    whole_mask = np.multiply(one_lesion_mask.astype(int), patches)\n",
    "    return whole_mask, all_centers\n",
    "\n",
    "def extract_lesions_from_segmentation(dataset_id, pat):\n",
    "    dataset = AVAILABLE_DATASETS[dataset_id]\n",
    "    path = dataset.get(return_type=\"filename\", subject=f\"{pat:03d}\", **CONTRASTS[\"SEGMENTATION\"])[0]\n",
    "    mask_data = nib.load(path)\n",
    "    mask = mask_data.get_fdata()\n",
    "    _, n_labels_original = ndimage.measurements.label(mask)\n",
    "    \n",
    "    # prepare folder to save\n",
    "    folder_name, pipeline = SPLIT_LESIONS_METADATA[VERSION][\"folder_name\"], SPLIT_LESIONS_METADATA[VERSION][\"pipeline\"]\n",
    "    create_folder_for_split_lesions(dataset_id, pat, folder_name, pipeline)\n",
    "    split_lesions_path = generate_BIDS_path(dataset_id, subject=f\"{pat:03d}\", scope=SPLIT_LESIONS_METADATA[VERSION][\"pipeline\"], suffix=SPLIT_LESIONS_METADATA[VERSION][\"suffix\"], acquisition=None, extension=\"nii.gz\")\n",
    "    \n",
    "    # we start with the splitting\n",
    "    counter_confluent = 0\n",
    "    counter_whole = 0\n",
    "    lesions = {}\n",
    "    final_mask = np.zeros_like(mask)\n",
    "    \n",
    "    labelled, num_objects = ndimage.measurements.label(mask)\n",
    "    for les in tqdm(range(1, num_objects+1)):\n",
    "        '''\n",
    "        1000 => whole\n",
    "        2000 => splitted (if does not fit in a patch of 32x32x32)\n",
    "        '''\n",
    "        one_lesion_mask = (labelled == les).astype(int)\n",
    "        vol = np.sum(one_lesion_mask)\n",
    "        if vol < VOLUME_FILTER:\n",
    "            # CLEANED because too small\n",
    "            continue\n",
    "        \n",
    "        objects = ndimage.find_objects(one_lesion_mask)\n",
    "        if(len(objects) != 1):\n",
    "            print(\"ERROR\")\n",
    "        x, y, z = objects[0]\n",
    "        biggest_dim = max(x.stop - x.start, y.stop - y.start, z.stop - z.start)\n",
    "        if biggest_dim <= DIM_FILTER: # whole\n",
    "            les = 1000 + counter_whole\n",
    "            counter_whole += 1\n",
    "            c = ndimage.measurements.center_of_mass(one_lesion_mask)\n",
    "            lesions[les] = {\"center\": (int(round(c[0])), int(round(c[1])), int(round(c[2])))}\n",
    "            final_mask[one_lesion_mask.astype(bool)] = int(les)\n",
    "        else: # CONFLUENT lesion\n",
    "            whole_mask, all_centers = __get_centers_of_sublesions(one_lesion_mask)\n",
    "            #all_centers = __get_centers_of_sublesions(one_lesion_mask)\n",
    "            whole_mask[whole_mask != 0] += 2000 + counter_confluent\n",
    "            final_mask += whole_mask\n",
    "            for center in all_centers:\n",
    "                lesions[2000 + counter_confluent] = {\"center\": center}\n",
    "                counter_confluent += 1\n",
    "    \n",
    "    \n",
    "    nifti_out = nib.Nifti1Image(final_mask, mask_data.affine, mask_data.header)\n",
    "    nifti_out.to_filename(split_lesions_path)\n",
    "    return dataset_id, pat, n_labels_original, len(lesions.keys())\n",
    "    #return final_mask, lesions\n",
    "\n",
    "def retrieve_all_patients_centers(cpus = 6):\n",
    "    pool = mp.Pool(min(cpus, mp.cpu_count()))\n",
    "    processes = []\n",
    "    results = []\n",
    "    for dataset_id in range(len(AVAILABLE_DATASETS)):\n",
    "        dataset = AVAILABLE_DATASETS[dataset_id]\n",
    "        for pat in dataset.get_subjects():\n",
    "            def callback(result):\n",
    "                if result is None:\n",
    "                    print(\"ERROR!\")\n",
    "                    return\n",
    "                elif result == -1:\n",
    "                    return\n",
    "                d_id, pat, bef, after = result\n",
    "                print(f\"{d_id}.{pat} - TOTAL: {bef} -> {after} lesions.\")\n",
    "                results.append(result)\n",
    "            processes.append(pool.apply_async(extract_lesions_from_segmentation, args=(dataset_id, int(pat)), callback=callback))\n",
    "\n",
    "    for p in processes:\n",
    "        p.get()\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_split_lesions_with_GT(dataset_id, pat):\n",
    "    np.random.seed(seed=0)\n",
    "    \n",
    "    #m = Munkres()\n",
    "    \n",
    "    folder_name, pipeline = SPLIT_LESIONS_METADATA[VERSION][\"folder_name\"], SPLIT_LESIONS_METADATA[VERSION][\"pipeline\"]\n",
    "    \n",
    "    # Loading of paths.\n",
    "    dataset = AVAILABLE_DATASETS[dataset_id]\n",
    "    split_lesions_path = generate_BIDS_path(dataset_id, subject=f\"{pat:03d}\", scope=SPLIT_LESIONS_METADATA[VERSION][\"pipeline\"], suffix=SPLIT_LESIONS_METADATA[VERSION][\"suffix\"], acquisition=None, extension=\"nii.gz\")\n",
    "    rimpos_annotations_paths = dataset.get(return_type=\"filename\", subject=f\"{pat:03d}\", **CONTRASTS[\"EXPERTS_ANNOTATIONS\"])\n",
    "    meta_split_lesions_path = generate_BIDS_path(dataset_id, subject=f\"{pat:03d}\", scope=SPLIT_LESIONS_METADATA[VERSION][\"pipeline\"], suffix=SPLIT_LESIONS_METADATA[VERSION][\"suffix\"], acquisition=None, extension=\"csv\")\n",
    "    \n",
    "    if not os.path.exists(split_lesions_path):\n",
    "        print(f\"Patient {dataset_id}.{pat} skipped because split lesions for version {VERSION} are missing.\")\n",
    "        return\n",
    "    if os.path.exists(meta_split_lesions_path):\n",
    "        print(f\"Patient {dataset_id}.{pat} skipped because metadata of split lesions for version {VERSION} already exists.\")\n",
    "        return\n",
    "    \n",
    "    # IF there are annotations of rim+, we match them with the lesions.\n",
    "    rimpos_centers = []\n",
    "    split_lesions_centers = []\n",
    "    \n",
    "    \n",
    "    # we retrieve them and make the association with split lesions\n",
    "    m_image = nib.load(split_lesions_path)\n",
    "    split_lesions = m_image.get_fdata()\n",
    "    if len(rimpos_annotations_paths) > 0:\n",
    "        gt_lesions = nib.load(rimpos_annotations_paths[0]).get_fdata()\n",
    "    else:\n",
    "        gt_lesions = np.zeros_like(split_lesions)\n",
    "    o, h = m_image.affine, m_image.header\n",
    "\n",
    "    labels_GT = np.unique(gt_lesions)\n",
    "    labels_GT = np.delete(labels_GT, np.argwhere(labels_GT == 0))\n",
    "    n_labels_GT = len(labels_GT)\n",
    "\n",
    "    # quick check that everything is okay\n",
    "    #assert n_labels_GT == len(df_lesions[(df_lesions[\"rim\"]) & (df_lesions[\"dataset\"] == dataset_id) & (df_lesions[\"patient\"] == pat)])\n",
    "    for lab in labels_GT:\n",
    "        rimpos_centers.append([int(el) for el in ndimage.measurements.center_of_mass(gt_lesions == lab)])\n",
    "\n",
    "    print(f\"{dataset_id} - {pat} - {len(rimpos_centers)} rim+ lesion centers extracted.\")\n",
    "\n",
    "    labels_split = np.unique(split_lesions)\n",
    "    labels_split = np.delete(labels_split, np.argwhere(labels_split == 0))\n",
    "    n_labels_split = len(labels_split)\n",
    "    for lab in labels_split:\n",
    "        split_lesions_centers.append([int(el) for el in ndimage.measurements.center_of_mass(split_lesions == lab)])\n",
    "\n",
    "    matrix_distances = np.zeros((len(rimpos_centers), len(split_lesions_centers)))\n",
    "    matrix_intersection = np.zeros((len(rimpos_centers), len(split_lesions_centers)))\n",
    "    matrix_intersection_perc = np.zeros((len(rimpos_centers), len(split_lesions_centers)))\n",
    "    for i in range(len(rimpos_centers)):\n",
    "        for j in range(len(split_lesions_centers)):\n",
    "            label_GT, label_lesions = i+1, j+1\n",
    "            # save the distance between them\n",
    "            c1, c2 = rimpos_centers[i], split_lesions_centers[j]\n",
    "            dist = np.linalg.norm(np.array(c1) - np.array(c2))\n",
    "            matrix_distances[i,j] = dist\n",
    "            # check for intersection\n",
    "            if dist < 15: # to optimize\n",
    "                current_les = (split_lesions == label_lesions).astype(int)\n",
    "                current_rim_slice = (gt_lesions == label_GT).astype(int)\n",
    "                matrix_intersection[i,j] = np.sum(current_les * current_rim_slice)\n",
    "            else:\n",
    "                matrix_intersection[i,j] = 0\n",
    "                matrix_intersection_perc[i,j] = 0\n",
    "\n",
    "    pos_patches = np.zeros_like(gt_lesions)\n",
    "    result = np.zeros_like(gt_lesions)\n",
    "    counter_rim = 1\n",
    "    counter_non_rim = 1\n",
    "    result_data = []\n",
    "    MAX_REAL = len(split_lesions_centers)\n",
    "    i = 0\n",
    "    while i < len(split_lesions_centers):\n",
    "        c = split_lesions_centers[i]\n",
    "        # we build the bounding box and get the percentage of rim+ slice inside\n",
    "        a = int(28/2) # PATCH SIZE = 28x28x28\n",
    "        patch = gt_lesions[max(0, c[0]-a):min(c[0]+a, gt_lesions.shape[0]), \n",
    "                           max(0, c[1]-a):min(c[1]+a, gt_lesions.shape[1]), \n",
    "                           max(0, c[2]-a):min(c[2]+a, gt_lesions.shape[2])]\n",
    "        perc_volume_rims_included_in_the_patch = [np.sum(patch == lab) / np.sum(gt_lesions == lab) for lab in np.unique(patch)[1:]]\n",
    "        voxels_rims_included_in_the_patch = [np.sum(patch == lab) for lab in np.unique(patch)[1:]]\n",
    "\n",
    "        maximum = 0 if len(perc_volume_rims_included_in_the_patch) == 0 else max(perc_volume_rims_included_in_the_patch)\n",
    "\n",
    "        # FULL OF HYPERPARAMETERS:\n",
    "        #     0.8 => we only want to augment those rim+ well fit inside the patch, so we can find patches around it that they still contain it.\n",
    "        #     2 + np.random.rand() * 8 => distance to which generate the augmented patches.\n",
    "        if i < MAX_REAL:# and maximum >= 0.8:\n",
    "            # DA\n",
    "            for x in (-1, 1):\n",
    "                for y in (-1, 1):\n",
    "                    for z in (-1, 1):\n",
    "                        if np.random.rand() > RANDOM_DA:\n",
    "                            split_lesions_centers.append((c[0] + x * int(np.ceil(2 + np.random.rand() * 8)), \n",
    "                                                          c[1] + y * int(np.ceil(2 + np.random.rand() * 8)), \n",
    "                                                          c[2] + z * int(np.ceil(2 + np.random.rand() * 8))))\n",
    "\n",
    "        b = 2\n",
    "        if maximum >= MIN_PERCENTAGE_FOR_RIM_POS_PATCHES / 100: # RIM+\n",
    "            result_data.append((dataset_id, pat, 1000 + counter_rim, c[0], c[1], c[2], perc_volume_rims_included_in_the_patch, voxels_rims_included_in_the_patch, i < MAX_REAL))\n",
    "\n",
    "            # for checking visually all lesions\n",
    "            result[c[0]-b:c[0]+b, c[1]-b:c[1]+b, c[2]-b:c[2]+b] = 1000 + counter_rim\n",
    "\n",
    "            # for checking visually the rim+ patches\n",
    "            pos_patches[max(0, c[0]-a):min(c[0]+a, gt_lesions.shape[0]), \n",
    "                       max(0, c[1]-a):min(c[1]+a, gt_lesions.shape[1]), \n",
    "                       max(0, c[2]-a):min(c[2]+a, gt_lesions.shape[2])] = counter_rim\n",
    "            a -= 1\n",
    "            pos_patches[max(0, c[0]-a):min(c[0]+a, gt_lesions.shape[0]), \n",
    "                       max(0, c[1]-a):min(c[1]+a, gt_lesions.shape[1]), \n",
    "                       max(0, c[2]-a):min(c[2]+a, gt_lesions.shape[2])] = 0\n",
    "\n",
    "            counter_rim += 1\n",
    "        elif i < MAX_REAL: # RIM- => we do not risk to create rim- patches close to rim+ ones\n",
    "            result_data.append((dataset_id, pat, 2000 + counter_non_rim, c[0], c[1], c[2], perc_volume_rims_included_in_the_patch, voxels_rims_included_in_the_patch, i < MAX_REAL))\n",
    "            # for checking visually all lesions\n",
    "            result[c[0]-b:c[0]+b, c[1]-b:c[1]+b, c[2]-b:c[2]+b] = 2000 + counter_rim\n",
    "            counter_non_rim += 1\n",
    "        i += 1\n",
    "\n",
    "    nifti_out = nib.Nifti1Image(pos_patches, affine = o, header = h)\n",
    "    nifti_out.to_filename(split_lesions_path.replace(\".nii.gz\", \"_rimpos.nii.gz\"))\n",
    "    nifti_out = nib.Nifti1Image(result, affine = o, header = h)\n",
    "    nifti_out.to_filename(split_lesions_path.replace(\".nii.gz\", \"_result.nii.gz\"))\n",
    "\n",
    "    df = pd.DataFrame(result_data, columns=[\"dataset_id\", \"patient\", \"lesion\", \"x\", \"y\", \"z\", \"percentage_rims\", \"voxels_rims\", \"real\"])\n",
    "    df.to_csv(meta_split_lesions_path, index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def match_all_patients_with_GT(cpus=6):\n",
    "    data = []\n",
    "    pool = mp.Pool(min(cpus, mp.cpu_count()))\n",
    "    processes = []\n",
    "    results = []\n",
    "    for dataset_id in range(len(AVAILABLE_DATASETS)):\n",
    "        dataset = AVAILABLE_DATASETS[dataset_id]\n",
    "        for pat in dataset.get_subjects():\n",
    "            def callback(result):\n",
    "                if result is None:\n",
    "                    #print(result)\n",
    "                    return\n",
    "                data.append(result)\n",
    "            processes.append(pool.apply_async(match_split_lesions_with_GT, args=(dataset_id, int(pat)), callback=callback))\n",
    "\n",
    "    for p in processes:\n",
    "        p.get()\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve_all_patients_centers()\n",
    "#match_all_patients_with_GT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_lesions\n",
    "\n",
    "lpp = load_lesions(PATCH_SIZE, from_segmentation=True, deformed=None, only_real=False, split_version=VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Subject 1 was omitted because no lesions 'json' file was found.\n",
      "[WARNING] Subject 2 was omitted because no lesions 'json' file was found.\n",
      "[WARNING] Subject 3 was omitted because no lesions 'json' file was found.\n",
      "[WARNING] Subject 4 was omitted because no lesions 'json' file was found.\n",
      "[WARNING] Subject 5 was omitted because no lesions 'json' file was found.\n",
      "[WARNING] Subject 6 was omitted because no lesions 'json' file was found.\n",
      "[WARNING] Subject 7 was omitted because no lesions 'json' file was found.\n",
      "[WARNING] Subject 8 was omitted because no lesions 'json' file was found.\n",
      "[WARNING] Subject 9 was omitted because no lesions 'json' file was found.\n",
      "[WARNING] Subject 10 was omitted because no lesions 'json' file was found.\n",
      "[WARNING] Subject 11 was omitted because no lesions 'json' file was found.\n",
      "[WARNING] Subject 12 was omitted because no lesions 'json' file was found.\n",
      "[WARNING] Subject 13 was omitted because no lesions 'json' file was found.\n",
      "[WARNING] Subject 14 was omitted because no lesions 'json' file was found.\n",
      "[WARNING] Subject 15 was omitted because no lesions 'json' file was found.\n",
      "[WARNING] Subject 16 was omitted because no lesions 'json' file was found.\n",
      "[WARNING] Subject 17 was omitted because no lesions 'json' file was found.\n",
      "[WARNING] Subject 18 was omitted because no lesions 'json' file was found.\n",
      "[WARNING] Subject 19 was omitted because no lesions 'json' file was found.\n",
      "[WARNING] Subject 20 was omitted because no lesions 'json' file was found.\n"
     ]
    }
   ],
   "source": [
    "from utils import get_dataframe_from_metadata\n",
    "\n",
    "df1 = get_dataframe_from_split_lesions(VERSION)\n",
    "df2 = get_dataframe_from_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0-56: + 0 -> 0, - 8 -> 8]\n",
      "[0-57: + 3 -> 3, - 2 -> 2]\n",
      "[0-58: + 1 -> 1, - 31 -> 26]\n",
      "[0-59: + 0 -> 0, - 10 -> 6]\n",
      "[0-60: + 1 -> 3, - 34 -> 36]\n",
      "[0-61: + 0 -> 0, - 115 -> 59]\n",
      "[0-62: + 0 -> 0, - 8 -> 7]\n",
      "[0-63: + 0 -> 0, - 28 -> 25]\n",
      "[0-64: + 11 -> 22, - 239 -> 196]\n",
      "[0-65: + 3 -> 3, - 140 -> 94]\n",
      "[0-66: + 4 -> 4, - 10 -> 7]\n",
      "[0-67: + 2 -> 2, - 16 -> 16]\n",
      "[0-68: + 0 -> 0, - 52 -> 31]\n",
      "[0-69: + 1 -> 1, - 1 -> 1]\n",
      "[0-70: + 6 -> 5, - 40 -> 30]\n",
      "[0-71: + 0 -> 0, - 69 -> 56]\n",
      "[0-72: + 3 -> 3, - 40 -> 22]\n",
      "[0-73: + 0 -> 0, - 53 -> 27]\n",
      "[0-74: + 6 -> 10, - 53 -> 36]\n",
      "[0-75: + 5 -> 4, - 51 -> 39]\n",
      "[0-76: + 0 -> 0, - 74 -> 49]\n",
      "[0-77: + 2 -> 3, - 24 -> 18]\n",
      "[0-78: + 8 -> 8, - 55 -> 34]\n",
      "[0-79: + 1 -> 1, - 91 -> 79]\n",
      "[0-80: + 1 -> 1, - 16 -> 12]\n",
      "[0-81: + 4 -> 3, - 74 -> 52]\n",
      "[0-82: + 0 -> 0, - 32 -> 17]\n",
      "[0-83: + 1 -> 1, - 81 -> 49]\n",
      "[0-84: + 18 -> 26, - 111 -> 59]\n",
      "[0-85: + 32 -> 43, - 99 -> 60]\n",
      "[0-92: + 9 -> 14, - 71 -> 45]\n",
      "[0-93: + 28 -> 31, - 145 -> 105]\n",
      "[0-94: + 9 -> 13, - 54 -> 34]\n",
      "[0-95: + 16 -> 28, - 102 -> 113]\n",
      "[0-96: + 3 -> 5, - 121 -> 96]\n",
      "[0-97: + 5 -> 9, - 85 -> 64]\n",
      "[0-98: + 0 -> 0, - 83 -> 67]\n",
      "[0-99: + 0 -> 0, - 3 -> 2]\n",
      "[0-100: + 0 -> 0, - 51 -> 29]\n",
      "[0-101: + 12 -> 16, - 83 -> 60]\n",
      "[0-102: + 6 -> 8, - 19 -> 15]\n",
      "[0-103: + 0 -> 0, - 63 -> 58]\n",
      "[0-104: + 0 -> 0, - 1 -> 1]\n",
      "[0-105: + 0 -> 0, - 7 -> 3]\n",
      "[0-106: + 0 -> 0, - 6 -> 5]\n",
      "[0-107: + 0 -> 0, - 22 -> 19]\n",
      "[0-108: + 14 -> 20, - 74 -> 43]\n",
      "[0-109: + 12 -> 17, - 122 -> 91]\n",
      "[0-110: + 0 -> 0, - 4 -> 4]\n",
      "[0-112: + 6 -> 5, - 30 -> 25]\n",
      "[0-113: + 2 -> 1, - 25 -> 22]\n",
      "[0-114: + 8 -> 16, - 80 -> 61]\n",
      "[0-115: + 5 -> 5, - 26 -> 13]\n",
      "[0-116: + 1 -> 1, - 6 -> 5]\n",
      "[0-117: + 1 -> 1, - 54 -> 44]\n",
      "[0-118: + 6 -> 13, - 143 -> 110]\n",
      "[0-120: + 0 -> 0, - 6 -> 6]\n",
      "[0-121: + 0 -> 0, - 16 -> 14]\n",
      "[0-122: + 1 -> 1, - 3 -> 1]\n",
      "[0-123: + 4 -> 6, - 90 -> 83]\n",
      "[0-124: + 10 -> 20, - 137 -> 72]\n",
      "[0-125: + 4 -> 5, - 70 -> 50]\n",
      "[0-126: + 1 -> 2, - 77 -> 53]\n",
      "[0-127: + 0 -> 0, - 18 -> 36]\n",
      "[0-128: + 1 -> 2, - 196 -> 130]\n",
      "[0-129: + 15 -> 26, - 120 -> 106]\n",
      "[0-130: + 14 -> 18, - 133 -> 91]\n",
      "[0-131: + 5 -> 9, - 36 -> 26]\n",
      "[0-132: + 2 -> 3, - 99 -> 91]\n",
      "[1-1: + 1 -> 0, - 24 -> 41]\n",
      "[1-2: + 0 -> 0, - 28 -> 23]\n",
      "[1-3: + 0 -> 0, - 9 -> 7]\n",
      "[1-4: + 0 -> 0, - 4 -> 4]\n",
      "[1-5: + 4 -> 6, - 25 -> 41]\n",
      "[1-6: + 0 -> 0, - 131 -> 160]\n",
      "[1-7: + 0 -> 0, - 12 -> 12]\n",
      "[1-8: + 1 -> 1, - 21 -> 20]\n",
      "[1-9: + 0 -> 0, - 34 -> 32]\n",
      "[1-10: + 2 -> 1, - 40 -> 40]\n",
      "[1-11: + 0 -> 0, - 29 -> 22]\n",
      "[1-12: + 0 -> 0, - 30 -> 30]\n",
      "[1-13: + 4 -> 4, - 33 -> 37]\n",
      "[1-14: + 1 -> 1, - 20 -> 15]\n",
      "[1-15: + 0 -> 0, - 11 -> 11]\n",
      "[1-16: + 0 -> 0, - 63 -> 70]\n",
      "[1-17: + 0 -> 0, - 11 -> 10]\n",
      "[1-18: + 1 -> 1, - 76 -> 74]\n",
      "[1-19: + 0 -> 0, - 12 -> 12]\n",
      "[1-20: + 1 -> 1, - 23 -> 24]\n",
      "[1-21: + 0 -> 0, - 9 -> 9]\n",
      "[1-22: + 1 -> 3, - 5 -> 6]\n",
      "[1-23: + 0 -> 0, - 10 -> 5]\n",
      "[1-24: + 0 -> 0, - 49 -> 61]\n",
      "[1-25: + 0 -> 0, - 25 -> 19]\n",
      "[1-26: + 0 -> 0, - 5 -> 5]\n",
      "[1-27: + 1 -> 1, - 11 -> 11]\n",
      "[1-28: + 0 -> 0, - 34 -> 36]\n",
      "[1-29: + 8 -> 11, - 82 -> 69]\n",
      "[1-30: + 0 -> 0, - 10 -> 9]\n",
      "[1-31: + 2 -> 3, - 59 -> 55]\n",
      "[1-32: + 1 -> 1, - 17 -> 23]\n",
      "[1-33: + 0 -> 0, - 44 -> 38]\n",
      "[1-34: + 1 -> 1, - 21 -> 24]\n",
      "[1-35: + 0 -> 0, - 36 -> 38]\n",
      "[1-36: + 0 -> 0, - 10 -> 10]\n",
      "[1-37: + 7 -> 11, - 76 -> 97]\n",
      "[1-38: + 0 -> 0, - 88 -> 92]\n",
      "[1-39: + 39 -> 47, - 90 -> 68]\n",
      "[1-40: + 3 -> 6, - 111 -> 116]\n",
      "[1-41: + 0 -> 0, - 48 -> 49]\n",
      "[1-42: + 1 -> 2, - 53 -> 58]\n",
      "[1-43: + 16 -> 37, - 109 -> 106]\n",
      "[1-44: + 0 -> 0, - 21 -> 28]\n",
      "[1-45: + 4 -> 7, - 39 -> 49]\n",
      "[1-46: + 0 -> 0, - 30 -> 29]\n",
      "[1-47: + 4 -> 7, - 118 -> 144]\n",
      "[1-48: + 6 -> 12, - 53 -> 79]\n",
      "[1-49: + 1 -> 2, - 107 -> 122]\n",
      "[1-50: + 0 -> 0, - 92 -> 95]\n",
      "[1-51: + 36 -> 38, - 68 -> 45]\n",
      "[1-52: + 1 -> 3, - 49 -> 82]\n",
      "[1-53: + 0 -> 0, - 94 -> 92]\n",
      "[1-54: + 1 -> 1, - 60 -> 55]\n",
      "[1-55: + 1 -> 1, - 22 -> 18]\n"
     ]
    }
   ],
   "source": [
    "for (db, pat), grouped in df2.groupby([\"dataset\", \"patient\"]):\n",
    "    rimpos_original = len(grouped[grouped[\"lesion\"] // 1000 == 1].index)\n",
    "    rimneg_original = len(grouped[grouped[\"lesion\"] // 2000 == 1].index)\n",
    "    df1_pat = df1[(df1[\"dataset_id\"] == db) & (df1[\"patient\"] == pat) & (df1[\"real\"])]\n",
    "    rimpos_now = len(df1_pat[df1_pat[\"lesion\"] // 1000 == 1].index)\n",
    "    rimneg_now = len(df1_pat[df1_pat[\"lesion\"] // 2000 == 1].index)\n",
    "    print(f\"[{db}-{pat}: + {rimpos_original} -> {rimpos_now}, - {rimneg_original} -> {rimneg_now}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>patient</th>\n",
       "      <th>lesion</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>percentage_rims</th>\n",
       "      <th>voxels_rims</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>2001</td>\n",
       "      <td>98</td>\n",
       "      <td>129</td>\n",
       "      <td>262</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>2002</td>\n",
       "      <td>97</td>\n",
       "      <td>238</td>\n",
       "      <td>263</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>2003</td>\n",
       "      <td>159</td>\n",
       "      <td>215</td>\n",
       "      <td>271</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>2004</td>\n",
       "      <td>162</td>\n",
       "      <td>222</td>\n",
       "      <td>266</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>2005</td>\n",
       "      <td>164</td>\n",
       "      <td>250</td>\n",
       "      <td>245</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>2014</td>\n",
       "      <td>159</td>\n",
       "      <td>262</td>\n",
       "      <td>218</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>2015</td>\n",
       "      <td>103</td>\n",
       "      <td>161</td>\n",
       "      <td>263</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>2016</td>\n",
       "      <td>146</td>\n",
       "      <td>145</td>\n",
       "      <td>239</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>2017</td>\n",
       "      <td>150</td>\n",
       "      <td>145</td>\n",
       "      <td>253</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>2018</td>\n",
       "      <td>148</td>\n",
       "      <td>194</td>\n",
       "      <td>265</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5573 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset_id  patient  lesion    x    y    z percentage_rims voxels_rims  \\\n",
       "0            0       56    2001   98  129  262              []          []   \n",
       "1            0       56    2002   97  238  263              []          []   \n",
       "2            0       56    2003  159  215  271              []          []   \n",
       "3            0       56    2004  162  222  266              []          []   \n",
       "4            0       56    2005  164  250  245              []          []   \n",
       "..         ...      ...     ...  ...  ...  ...             ...         ...   \n",
       "13           1       55    2014  159  262  218              []          []   \n",
       "15           1       55    2015  103  161  263              []          []   \n",
       "16           1       55    2016  146  145  239              []          []   \n",
       "17           1       55    2017  150  145  253              []          []   \n",
       "18           1       55    2018  148  194  265              []          []   \n",
       "\n",
       "    real  \n",
       "0   True  \n",
       "1   True  \n",
       "2   True  \n",
       "3   True  \n",
       "4   True  \n",
       "..   ...  \n",
       "13  True  \n",
       "15  True  \n",
       "16  True  \n",
       "17  True  \n",
       "18  True  \n",
       "\n",
       "[5573 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.loc[(df1[\"lesion\"] // 2000 == 1) & (df1[\"dataset_id\"] != 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>patient</th>\n",
       "      <th>lesion</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>percentage_rims</th>\n",
       "      <th>voxels_rims</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>1001</td>\n",
       "      <td>89</td>\n",
       "      <td>251</td>\n",
       "      <td>238</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[655]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>1002</td>\n",
       "      <td>100</td>\n",
       "      <td>219</td>\n",
       "      <td>260</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[298]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>1003</td>\n",
       "      <td>106</td>\n",
       "      <td>130</td>\n",
       "      <td>277</td>\n",
       "      <td>[0.9867330016583747]</td>\n",
       "      <td>[1190]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>1004</td>\n",
       "      <td>92</td>\n",
       "      <td>228</td>\n",
       "      <td>254</td>\n",
       "      <td>[0.9865771812080537]</td>\n",
       "      <td>[294]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>1005</td>\n",
       "      <td>82</td>\n",
       "      <td>248</td>\n",
       "      <td>231</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[655]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>1004</td>\n",
       "      <td>95</td>\n",
       "      <td>224</td>\n",
       "      <td>273</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[135]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>1005</td>\n",
       "      <td>105</td>\n",
       "      <td>213</td>\n",
       "      <td>260</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[135]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>1006</td>\n",
       "      <td>105</td>\n",
       "      <td>212</td>\n",
       "      <td>275</td>\n",
       "      <td>[0.9111111111111111]</td>\n",
       "      <td>[123]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>1007</td>\n",
       "      <td>110</td>\n",
       "      <td>229</td>\n",
       "      <td>256</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[135]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>1008</td>\n",
       "      <td>111</td>\n",
       "      <td>229</td>\n",
       "      <td>271</td>\n",
       "      <td>[0.9259259259259259]</td>\n",
       "      <td>[125]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5401 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset_id  patient  lesion    x    y    z       percentage_rims  \\\n",
       "1            0       57    1001   89  251  238                 [1.0]   \n",
       "2            0       57    1002  100  219  260                 [1.0]   \n",
       "4            0       57    1003  106  130  277  [0.9867330016583747]   \n",
       "5            0       57    1004   92  228  254  [0.9865771812080537]   \n",
       "6            0       57    1005   82  248  231                 [1.0]   \n",
       "..         ...      ...     ...  ...  ...  ...                   ...   \n",
       "21           1       55    1004   95  224  273                 [1.0]   \n",
       "22           1       55    1005  105  213  260                 [1.0]   \n",
       "23           1       55    1006  105  212  275  [0.9111111111111111]   \n",
       "24           1       55    1007  110  229  256                 [1.0]   \n",
       "25           1       55    1008  111  229  271  [0.9259259259259259]   \n",
       "\n",
       "   voxels_rims   real  \n",
       "1        [655]   True  \n",
       "2        [298]   True  \n",
       "4       [1190]   True  \n",
       "5        [294]  False  \n",
       "6        [655]  False  \n",
       "..         ...    ...  \n",
       "21       [135]  False  \n",
       "22       [135]  False  \n",
       "23       [123]  False  \n",
       "24       [135]  False  \n",
       "25       [125]  False  \n",
       "\n",
       "[5401 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.loc[(df1[\"lesion\"] // 1000 == 1) & (df1[\"dataset_id\"] != 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Loading lesions (deformed=None, asyncr=True, cpus=8, segm=True)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-14:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lpp = load_lesions(PATCH_SIZE, from_segmentation=True, deformed=None, only_real=False, split_version=VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
